# -*- coding: utf-8 -*-
"""baabaAmosah__SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUHaTJ42OHZ5qiim4HZBILIxDv5c07vO
"""

pip install scikit-learn==1.5.0

"""# Import Libraries"""

# load datasets, Sanity Check and EDA
import pandas as pd
import numpy as np
import matplotlib as plt
from google.colab import drive
drive.mount('/content/drive')

# Missing Value Treatment, Encoding and Scaling
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# check which subset produces the best features
from xgboost import XGBRegressor

# model training and evaluation
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
import joblib

"""# Data Processing Functions

### Load Data
"""

# Iterate over the file in chunks
def readChunks(file_path):
    chunks = []
    for chunk in pd.read_csv(file_path, chunksize=1000):
        # Append each chunk to the list
        chunks.append(chunk)
    return pd.concat(chunks, ignore_index=True)

"""### clean data

"""

# drop columns with null > 30%
def null_cols(data):
    cols_to_drop = []
    for col in data.columns:
        if (data[col].isnull().sum()/data.shape[0])*100 > 30:
            cols_to_drop.append(col)
    data.drop(columns=cols_to_drop, inplace=True)

# drop unneccessary columns from dataset because it does neccessarily affect the player's overall rating
def unneccessary_cols(data):
    col_to_drop = ['player_url', 'short_name', 'long_name', 'league_name', 'league_level', 'club_team_id', 'club_name', 'club_position', 'club_jersey_number',
                   'nationality_id', 'nationality_name', 'real_face', "player_face_url", "dob", "player_positions"]

    if data.shape[0] == 161583:
        data.drop(columns=['fifa_version', 'fifa_update', 'fifa_update_date', 'league_id', 'club_joined_date', 'club_contract_valid_until_year', 'player_id'], inplace=True)
    else:

        data.drop(columns='sofifa_id', inplace=True)

    data.drop(columns=col_to_drop, inplace=True)

# function to extract base rating from the format 'rating+modifier'
# although it is the player's rating at that postion (assuming) this would not directly affect the player's overall rating
def LSGK(data):
    positions = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
    'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']
    data.drop(columns=positions, inplace=True)

"""### check correlation (drop columns with low correlation)"""

# check the correlation of values
def check_corr(data):
    low_corr = []

    # check the correlation between the target and the independent variable
    corr_matrix = data.select_dtypes(include=np.number).corr()

    corr_overall = corr_matrix["overall"].sort_values(ascending=False)
    # pick the features with correlation above 0.3
    for col in corr_matrix.index:
        if col != "overall" and abs(corr_overall[col]) < 0.3:
            low_corr.append(col)
    data.drop(columns=low_corr, axis=1, inplace=True)

"""### Imputation and Encoding: Separate Numeric from non-numeric

"""

# separate numeric and non-numeric and impute
def separate_cat_num(data):
    non_numeric = data.select_dtypes(include="object")
    numeric = data.select_dtypes(include=np.number)

    imp = IterativeImputer(max_iter=10, random_state=0)
    simp = SimpleImputer(strategy="most_frequent")

    imputed_numeric = pd.DataFrame(np.round(imp.fit_transform(numeric)), columns=numeric.columns)
    imputed_cat = pd.DataFrame(simp.fit_transform(non_numeric), columns=non_numeric.columns)

    return imputed_cat, imputed_numeric

# encode categorical
def encode_cat(categorical):
    encoder = OneHotEncoder()
    encoded = (encoder.fit_transform(categorical)).toarray()
    return pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical.columns))

# join numeric and categorical data into single dataframe
def merge_data(categorical, numeric):
    return pd.concat([numeric, categorical], axis=1)

"""# Feature Importance function"""

# find feature importance of dataset
def featureImportance(data):
    y = data["overall"]
    X = data.drop(columns="overall")

    xgb = XGBRegressor()

    # split data into training and testing
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.2, random_state=42)

    # fit transform
    xgb.fit(Xtrain, Ytrain)

    # get the feature importance
    feature_importances = xgb.feature_importances_

    feature_importances_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances})

    return feature_importances_df

"""# Start Work"""

def dataProcessing(file_path):
    # get the data set
    data = readChunks(file_path)

    # remove unneccessary data and clean columns LS to GK
    # remove null > 30%
    null_cols(data)
    unneccessary_cols(data)
    LSGK(data)

    # check for numeric features with high correlation with "overall"
    # remove columns with < 30% correlation
    check_corr(data)

    # replace missing values using IterativeImputer and SimpleImputer
    imputed_cat, imputed_numeric = separate_cat_num(data)
    categorical = encode_cat(imputed_cat)

    return merge_data(categorical, imputed_numeric)

data = dataProcessing("/content/drive/My Drive/Colab Notebooks/RegressionProblem/male_players.csv")

data.info()

"""# Feature Importance"""

# get the important features using gradient boosting
fi = featureImportance(data)

most_important = fi.sort_values("Importance", ascending=False)

# remove features with less importance to overall
most_important.head(15)

top_features = most_important.loc[:, "Feature"].head(10).tolist()

top_features

y = data["overall"]

X = data[top_features]

X

"""# scale function"""

# split target variable from independent variables and scale
def scale_independent(X):
    sc = StandardScaler()
    scaled = sc.fit_transform(X)
    return scaled

X = scale_independent(X)

X.shape

"""# 3. Created and Trained models with cross validation either RandomForest, XGBoost, GradientBoost Regressors."""

# set cross-validation and split and train each split on model
def cross_validation(model, X, y, n_splits=5):
  kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

  # perform k-fold cross-validation
  results = cross_val_score(model, X, y, cv=kf, scoring="neg_mean_squared_error")

  print(f"Cross Results: {results}")
  print(f"Neg Mean Squred Error (Overall) {results.mean()}")

# initialise random forest regressor with some preset paramters
rf = RandomForestRegressor(n_estimators=50, random_state=42)

cross_validation(rf, X, y)

# preset paramters to check for model performance
xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)

cross_validation(xgb, X, y)

# preset hyperparamters to understand the model performance
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)

cross_validation(gb, X, y)

"""###### From this evaluation it can be observed that random forest performs better than the other models having the least difference between its predictions and actual values. It also performs better than the others with individual splits. Using Hyperparameter tuning I will check if the R2 score and other evaluation metrics can be adjusted.

# 4. Used MAE or RMSE and then fine tune model, train, and tested again. Grid search (hyperparameter tuning)
"""

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.2, random_state=42)

# use cross-validation to split the data
cv = KFold(n_splits=5, shuffle=True, random_state=42)

"""###### Random Forest Ensemble"""

# random forest ensemle model
rf = RandomForestRegressor(random_state=42)

# vary the number of n_esitmators
param_rf = {'n_estimators': [50, 100, 200]}

model_rf = GridSearchCV(estimator=rf, param_grid=param_rf, cv=cv, scoring="neg_mean_squared_error")

model_rf.fit(Xtrain, Ytrain)

print(f"Scoring: {model_rf.cv_results_}")

# Save the model to the file
joblib.dump(model_rf, open("/content/drive/My Drive/" + rf.__class__.__name__ + ".joblib", "wb"))

y_pred = model_rf.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred, Ytest)},
Mean Squared Error = {mean_squared_error(y_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
""")

"""###### XGBoostRegressor"""

# initialise XGBoost
xgb = XGBRegressor()

param_xgb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]}

model_xgb = GridSearchCV(xgb, param_xgb, cv=cv, scoring="r2")

model_xgb.fit(Xtrain, Ytrain)

# Save the model to the file
joblib.dump(model_xgb, open("/content/drive/My Drive/" + xgb.__class__.__name__ + ".joblib", "wb"))

y_pred = model_xgb.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred, Ytest)},
Mean Squared Error = {mean_squared_error(y_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
""")

"""###### GradientBoostRegressor"""

# initialise the gradient
gb = GradientBoostingRegressor(n_iter_no_change=10, validation_fraction=0.1)

param_gb = {
    "max_depth": [3,5],
    "min_samples_split": [1,5],
    "min_samples_split": [2,5],
    "learning_rate": [0.5],
    "n_estimators": [100]
}

model_gb = GridSearchCV(gb, param_grid=param_gb, cv=cv, scoring="neg_mean_squared_error")

# due to large dataset and slowness of Gradient Boost Regressor
# split the train data into chunks and train model to each chunk
model_gb.fit(Xtrain, Ytrain)

# Save the model to the file
joblib.dump(model_gb, open("/content/drive/My Drive/" + gb.__class__.__name__ + ".joblib", "wb"))

y_pred = model_gb.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred, Ytest)},
Mean Squared Error = {mean_squared_error(y_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred, Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
""")

"""# Test with New Dataset Using Pipeline"""

players_22 = readChunks("/content/drive/My Drive/Colab Notebooks/RegressionProblem/players_22.csv")

# get target variable
y = players_22["overall"]

# get indepent variable
X = players_22[top_features]

X.info()

y.info()

# since all the top_feaatures are numeric I will use an iterative imputer
imp = IterativeImputer(max_iter=10, random_state=0)
X= pd.DataFrame(np.round(imp.fit_transform(X)), columns=X.columns)

X.isnull().sum()

X = scale_independent(X)

# using the R2 score find the best model grid
with open("/content/drive/My Drive/RandomForestRegressor.joblib", "rb") as file:
    best_model_Grid = joblib.load(file)

# best_model = best_model_Grid.best_estimator_

y_test_pred = best_model_Grid.predict(X)

print(f"""
Mean Absolute Error = {mean_absolute_error(y_test_pred, y)},
Mean Squared Error = {mean_squared_error(y_test_pred, y)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_test_pred, y))},
R2 Score = {r2_score(y_test_pred, y)}
""")